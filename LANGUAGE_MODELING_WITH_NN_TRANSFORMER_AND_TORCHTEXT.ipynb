{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TgkLhO_YH_2a"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "VKHyYzCFIjMm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install portalocker\n",
        "pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7eiv0GXImun",
        "outputId": "d7fa3a25-5e43-440d-fe0e-3fcd9815b52f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (17.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Arguments:\n",
        "        data: Tensor, shape ``[N]``\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape ``[N // bsz, bsz]``\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "Ng7nFibiIrBx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
        "        target has shape ``[seq_len * batch_size]``\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "9wvLKzh_ItTf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "id": "kz1YYi34IyoZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        output = model(data)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        loss = criterion(output_flat, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "_irxKMmWI0D4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "\n",
        "with TemporaryDirectory() as tempdir:\n",
        "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model)\n",
        "        val_loss = evaluate(model, val_data)\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZnG3ZaRI2cO",
        "outputId": "87e89345-d888-44b3-8da7-d4c82cf6c05e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 14.80 | loss  8.15 | ppl  3462.10\n",
            "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 13.61 | loss  6.28 | ppl   534.14\n",
            "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 19.82 | loss  5.67 | ppl   288.86\n",
            "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 21.39 | loss  5.45 | ppl   233.08\n",
            "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 18.57 | loss  5.22 | ppl   184.57\n",
            "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 14.55 | loss  4.87 | ppl   130.85\n",
            "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 14.15 | loss  4.46 | ppl    86.32\n",
            "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 13.85 | loss  4.22 | ppl    68.04\n",
            "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 13.84 | loss  3.93 | ppl    50.76\n",
            "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 13.95 | loss  3.80 | ppl    44.71\n",
            "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 14.29 | loss  3.59 | ppl    36.28\n",
            "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 14.00 | loss  3.59 | ppl    36.24\n",
            "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 14.03 | loss  3.50 | ppl    33.27\n",
            "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 14.04 | loss  3.37 | ppl    28.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 46.84s | valid loss  1.94 | valid ppl     6.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 14.21 | loss  3.17 | ppl    23.90\n",
            "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 14.20 | loss  3.07 | ppl    21.55\n",
            "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 14.23 | loss  2.92 | ppl    18.54\n",
            "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 14.53 | loss  2.91 | ppl    18.41\n",
            "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 14.37 | loss  2.79 | ppl    16.27\n",
            "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 14.29 | loss  2.77 | ppl    15.90\n",
            "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 14.29 | loss  2.77 | ppl    15.93\n",
            "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 14.33 | loss  2.76 | ppl    15.74\n",
            "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 14.40 | loss  2.67 | ppl    14.47\n",
            "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 14.17 | loss  2.69 | ppl    14.67\n",
            "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 14.14 | loss  2.63 | ppl    13.87\n",
            "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 14.34 | loss  2.72 | ppl    15.12\n",
            "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 18.08 | loss  2.66 | ppl    14.30\n",
            "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 14.07 | loss  2.65 | ppl    14.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 44.38s | valid loss  1.37 | valid ppl     3.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 14.24 | loss  2.47 | ppl    11.83\n",
            "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 14.28 | loss  2.47 | ppl    11.80\n",
            "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 14.03 | loss  2.42 | ppl    11.21\n",
            "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 14.03 | loss  2.39 | ppl    10.95\n",
            "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 14.01 | loss  2.30 | ppl     9.93\n",
            "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 14.40 | loss  2.43 | ppl    11.42\n",
            "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 13.98 | loss  2.36 | ppl    10.56\n",
            "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 14.04 | loss  2.36 | ppl    10.58\n",
            "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 14.00 | loss  2.31 | ppl    10.05\n",
            "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 14.38 | loss  2.33 | ppl    10.30\n",
            "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 14.08 | loss  2.26 | ppl     9.58\n",
            "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 14.07 | loss  2.34 | ppl    10.40\n",
            "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 14.02 | loss  2.37 | ppl    10.68\n",
            "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 14.20 | loss  2.50 | ppl    12.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 43.13s | valid loss  1.19 | valid ppl     3.27\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vmyMgM5I5aT",
        "outputId": "fe01b0c1-baeb-4447-db83-ce3a2e27b268"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  1.18 | test ppl     3.24\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eacN3zwFI7-n"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}